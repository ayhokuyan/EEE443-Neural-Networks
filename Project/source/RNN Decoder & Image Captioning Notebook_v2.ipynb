{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EEE 443 - Final Project - Image Captioning:\n",
    "\n",
    "## Group 10:\n",
    "\n",
    "Ayhan Okuyan, Baris Akcin, Emre Donmez, Hasan Emre Erdemoglu, Ruzgar Eserol, Suleyman Taylan Topaloglu\n",
    "\n",
    "### RNN Decoder & Image Captioning Notebook: (Part 2 of 2)\n",
    "\n",
    "Note that in this section GPU will be utilized as training will be a cumbersome operation for the CPU. Some code is written to identify my GPU. \n",
    "\n",
    "1. Validate which directories that you work at, then import given dataset. The data extracted from the first notebook will be unpickled here. For each of the transfer learning encoder models (CNN models) that is extracted by Notebook 1, an RNN network can be tied to. \n",
    "\n",
    "    \n",
    "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of GPUs Available:  1\n",
      "\n",
      "\n",
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 5586630614588554977\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 4937233203\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 8443299653674864976\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 1060 with Max-Q Design, pci bus id: 0000:01:00.0, compute capability: 6.1\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf # tensorflow gpu capabilities available\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "print(\"Number of GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "print('\\n')\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ayhok\\Desktop\\EEE443 Project\n",
      "['.ipynb_checkpoints', 'Attention.ipynb', 'eee443_project_dataset_train.h5', 'exports', 'Image Downloader & Pickler.ipynb', 'img.zip', 'img_encodings.pkl', 'img_encodings_indices.pkl', 'RNN Decoder & Image Captioning Notebook_v2.ipynb', 'word_dict.pkl']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "root_dir = os.getcwd()\n",
    "exp_dir = root_dir\n",
    "\n",
    "# Fetch and display the directory that we are working on:\n",
    "print(os.getcwd())\n",
    "print(os.listdir())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Extraction & Unpickling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "def eee443_dataset_read(path):\n",
    "    f = h5py.File(path + '\\\\eee443_project_dataset_train.h5', 'r')\n",
    "    train_cap = f['train_cap']\n",
    "    train_imid = f['train_imid']\n",
    "    train_url = f['train_url']\n",
    "    word_code = f['word_code']\n",
    "    train_ims = None\n",
    "    return train_imid, train_cap, train_url, word_code, f\n",
    "\n",
    "def extract_word_dict(dataset):\n",
    "    words_struct = dataset.get('word_code')[()]\n",
    "    word_list = list(words_struct.dtype.names) # returns the key\n",
    "    w_all_idx = []\n",
    "    for word in word_list:\n",
    "        w_idx = int(words_struct[word])\n",
    "        w_all_idx.append(w_idx)\n",
    "\n",
    "    #print(w_all_idx)\n",
    "    \n",
    "    word_dict = dict(zip(w_all_idx, word_list))\n",
    "    #print(word_dict[627]) # shop\n",
    "    return word_dict\n",
    "\n",
    "def unpickle_data(path, filename):\n",
    "    file = pickle.load(open(path + filename,'rb'), encoding='utf8')\n",
    "    return file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset Reading & PKL File Extraction for CNN Encoders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of words dictionary:  1004\n"
     ]
    }
   ],
   "source": [
    "# Dataset reading: - Export word_dict too.\n",
    "_, _, _, _, f = eee443_dataset_read(root_dir) # from previous notebook.\n",
    "word_dict = extract_word_dict(f)\n",
    "\n",
    "# Print if everything is alright or not.\n",
    "print('Size of words dictionary: ', len(word_dict.keys()))\n",
    "#print(word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "(73724, 1, 2048)\n",
      "Encoded images final shape:  (73724, 2048)\n",
      "\n",
      "\n",
      "<class 'list'>\n",
      "(73724,)\n",
      "Encoded image indices final shape:  (73724,)\n",
      "60\n",
      "\n",
      "\n",
      "Inception encoding dictionary with query 10:  [0.0498227  0.10677398 0.05705195 ... 0.75969905 0.36796963 0.        ]\n",
      "Size of inception encoding dictionary with query 10:  2048\n",
      "73724\n"
     ]
    }
   ],
   "source": [
    "# Inception Data Unpickling:\n",
    "enc_inception = unpickle_data(exp_dir, '\\\\img_encodings.pkl')\n",
    "enc_inception_idx = unpickle_data(exp_dir, '\\\\img_encodings_indices.pkl')\n",
    "\n",
    "# Verify sizes and all sorts of stuff:\n",
    "print(type(enc_inception))\n",
    "print(np.shape(enc_inception))\n",
    "enc_inception = np.squeeze(enc_inception)\n",
    "print('Encoded images final shape: ', np.shape(enc_inception))\n",
    "print('\\n')\n",
    "\n",
    "print(type(enc_inception_idx))\n",
    "print(np.shape(enc_inception_idx))\n",
    "enc_inception_idx = np.squeeze(enc_inception_idx)\n",
    "print('Encoded image indices final shape: ', np.shape(enc_inception_idx))\n",
    "print(len(enc_inception_idx[300:360]))\n",
    "print('\\n')\n",
    "\n",
    "enc_inception_dict = dict(zip(enc_inception_idx, enc_inception))\n",
    "print('Inception encoding dictionary with query 10: ', enc_inception_dict['10']) # 10 is the 10.jpg here. \n",
    "print('Size of inception encoding dictionary with query 10: ', len(enc_inception_dict['10'])) # 10 is the 10.jpg here. \n",
    "print(len(enc_inception_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VGG16 Data Unpickling: (to be implemented)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inception ResNet V2 Data Unpickling: (to be implemented)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caption Helper Functions:\n",
    "\n",
    "These functions are used to extract numerical and textual captions from the given dataset. These will be useful in the later stages of the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Check the text data for images:\n",
    "# Extracting data from dataset:\n",
    "def fetch_captions(image_id, dataset, word_dict):\n",
    "    # Query this on train_imid to extract which indices hold the captions for this image:\n",
    "    imid = np.array(dataset.get('train_imid')[()])\n",
    "    indices = np.where(imid == int(image_id))[0] # since everything is string, must be cast to int manually.\n",
    "                                                 # indices is a tuple of array \n",
    "\n",
    "    # Extract the list of integer captions for the given image  \n",
    "    all_caps = np.array(dataset.get('train_cap')[()])\n",
    "    #print('Overall shape of the captions: ', all_caps.shape) \n",
    "    \n",
    "    count = 1\n",
    "    caps = []\n",
    "    for idx in indices:\n",
    "        cap = all_caps[idx][:]\n",
    "        #print('Caption ', str(count), ': ', str(cap))\n",
    "        caps.append(cap)\n",
    "        count += 1\n",
    "    \n",
    "    #print('')\n",
    "    #print('Captions: ', str(caps))  # Final look at the captions\n",
    "    \n",
    "    # Now do conversion:\n",
    "    text_cap = []\n",
    "    count = 0\n",
    "    for item in caps:\n",
    "        temp = []\n",
    "        count += 1\n",
    "        for word in item:\n",
    "            #print(item)\n",
    "            #print(str(word)  , (word_dict[word]))\n",
    "            temp.append(word_dict[word])\n",
    "        text_cap.append(temp)\n",
    "        #print('Caption ', count, ' textual: ', (' '.join(map(str, temp))).split('x_NULL_')[0]) # list comprehension\n",
    "        #temp.remove()\n",
    "    \n",
    "    #print(type(text_cap))\n",
    "    # Return captions     \n",
    "    caps = [list(c) for c in caps] # list comprehension to make arrays list\n",
    "    return indices, text_cap, caps\n",
    "\n",
    "# Helper function to print captions of an image:\n",
    "def print_captions(caps, word_dict):\n",
    "    # Only for displaying\n",
    "    #print('Indices of captions for this image:', test_id)\n",
    "    #print('\\n')\n",
    "\n",
    "    i = 0\n",
    "    text_cap = []\n",
    "    for item in caps:\n",
    "        i += 1\n",
    "        temp = []\n",
    "        print('Caption ', i, ': ',  item)\n",
    "        for word in item:\n",
    "            #print(item)\n",
    "            #print(str(word)  , (word_dict[word]))\n",
    "            temp.append(word_dict[word])\n",
    "        text_cap.append(temp)\n",
    "        # x_NULL_ strings are only ignored, not erased from the captions\n",
    "        print('Caption ', i, ' textual: ', (' '.join(map(str, temp))).split('x_NULL_')[0])\n",
    "        print('\\n')\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12376 12425 12565 12689 12809]\n",
      "Type of captions:  <class 'list'>\n",
      "Length of captions list:  5\n",
      "Type of one of the tokenized captions:  <class 'list'>\n",
      "[1, 16, 19, 8, 4, 61, 125, 107, 72, 18, 15, 3, 2, 0, 0, 0, 0]\n",
      "<class 'list'>\n",
      "[1, 4, 12, 8, 4, 3, 10, 254, 3, 93, 4, 185, 2, 0, 0, 0, 0]\n",
      "<class 'list'>\n",
      "[1, 4, 12, 8, 4, 61, 562, 72, 32, 18, 4, 238, 6, 328, 2, 0, 0]\n",
      "<class 'list'>\n",
      "[1, 4, 12, 9, 4, 3, 10, 4, 60, 189, 11, 460, 30, 61, 2, 0, 0]\n",
      "<class 'list'>\n",
      "[1, 4, 28, 507, 143, 7, 185, 6, 4, 61, 125, 2, 0, 0, 0, 0, 0]\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "indices, text_cap, caps = fetch_captions(10, f, word_dict)\n",
    "\n",
    "# Validation using image file: '10.jpg'\n",
    "# Printing the image itself or its encoded output doesn't matter.\n",
    "print(indices)\n",
    "#print_captions(caps,word_dict)\n",
    "\n",
    "print('Type of captions: ', type(caps))\n",
    "print('Length of captions list: ', len(caps))\n",
    "print('Type of one of the tokenized captions: ', type(caps[0]))\n",
    "\n",
    "for c in caps:\n",
    "    print(c)\n",
    "    print(type(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate training, validation and test sets:\n",
    "\n",
    "1. Split to 80 % training, 10 % validation and 10% test data.\n",
    "2. Upon successful training and validation train with 90 % of data keeping remaining 10% test data spared.\n",
    "3. Do as much as random picking for the image encodings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "\n",
    "def divide_into_two(percentage, list_to_divide):\n",
    "    indices = random.sample(range(1, len(list_to_divide)), len(list_to_divide)-math.ceil((1-percentage)*len(list_to_divide)))\n",
    "    samples = []\n",
    "    indices.sort(reverse=True) # reverse to do not alter original indices when removed\n",
    "    for idx in indices:\n",
    "            samples.append(list_to_divide[idx])\n",
    "            list_to_divide.remove(list_to_divide[idx])\n",
    "    # return back samples to original format\n",
    "    samples.reverse()\n",
    "    return samples, list_to_divide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 58979\n",
      "Validation size: 7372\n",
      "Test size: 7373\n",
      "Train sample: ['10526', '10528', '10529', '1053', '10530', '10531', '10533', '10534', '10536', '10537']\n",
      "Validation sample: ['14468', '14483', '14487', '14493', '14498', '14505', '14518', '1455', '1456', '14635']\n",
      "Test sample: ['14058', '14108', '14114', '1412', '14127', '14134', '14149', '14176', '14193', '14194']\n"
     ]
    }
   ],
   "source": [
    "TRAIN_PERCENTAGE = 0.8 # 80% train data, rest is 20% which is to be divided 10%/10%\n",
    "VAL_PERCENTAGE = 0.5 # to seperate out 50% of remaining data\n",
    "\n",
    "key_list = list(enc_inception_dict.keys())\n",
    "\n",
    "train, rest = divide_into_two(TRAIN_PERCENTAGE, key_list)\n",
    "validation, test = divide_into_two(VAL_PERCENTAGE, rest)\n",
    "print('Train size:', len(train))\n",
    "print('Validation size:',len(validation))\n",
    "print('Test size:',len(test))\n",
    "\n",
    "# Do further tests on training validation and test indices:\n",
    "print('Train sample:', train[400:410])\n",
    "print('Validation sample:', validation[400:410])\n",
    "print('Test sample:', test[400:410])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the input data:\n",
    "\n",
    "Each key with train, validation and test contains keys for encoding dictionary. Each key in the encoding corresponds to a single encoded image, and each encoded image corresponds to approximately 5 captions.\n",
    "\n",
    "1. For each key:\n",
    "    1.1. Replicate the encoding number of caption times. The size should be (5*key_size, 2048).\n",
    "    1.2. Store all captions in a singly 2D list. The size should be (5*key_size, 2048).\n",
    "2. Both encoding list and caption list will be aligned and these will be fed to the RNN decoder consecutively.\n",
    "\n",
    "(In further implementations, to benefit from the GPU, we will try to get these inputs by batches, so this will be the final preperation before feeding the input to the network.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def prepare_data(dataset, encoding_dict, data_key_list, word_dict):\n",
    "    encoding_dataset = []\n",
    "    caption_dataset = []\n",
    "    for key in tqdm(data_key_list): # Iterate over each key item in train/validation/test\n",
    "        _, _, key_captions = fetch_captions(key, dataset, word_dict)\n",
    "        len_captions_set = len(key_captions)\n",
    "        # Replicate encoding number of caption times.\n",
    "        for i in range(0,len_captions_set):\n",
    "            encoding_dataset.append(encoding_dict[key])\n",
    "        \n",
    "        # Process the captions:\n",
    "        for c in key_captions:\n",
    "            caption_dataset.append(c)\n",
    "            \n",
    "    return encoding_dataset,caption_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 7373/7373 [10:32<00:00, 11.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35681\n",
      "35681\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "encoding_dataset,caption_dataset = prepare_data(f, enc_inception_dict, test, word_dict)\n",
    "\n",
    "#Tests\n",
    "print(len(encoding_dataset))\n",
    "print(len(caption_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 58979/58979 [42:13<00:00, 23.28it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 7372/7372 [03:26<00:00, 35.76it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 7373/7373 [03:26<00:00, 35.66it/s]\n"
     ]
    }
   ],
   "source": [
    "# This process takes a long time.\n",
    "enc_trn,cap_trn = prepare_data(f, enc_inception_dict, train, word_dict)\n",
    "enc_val,cap_val = prepare_data(f, enc_inception_dict, validation, word_dict)\n",
    "enc_tst,cap_tst = prepare_data(f, enc_inception_dict, test, word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "285041\n",
      "285041 None\n",
      "35684\n",
      "35684 None\n",
      "35681\n",
      "35681 None\n"
     ]
    }
   ],
   "source": [
    "# Check sizes on datasets - proofread material:\n",
    "#Tests\n",
    "print(len(enc_trn),print(len(cap_trn)))\n",
    "print(len(enc_val),print(len(cap_val)))\n",
    "print(len(enc_tst),print(len(cap_tst)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n"
     ]
    }
   ],
   "source": [
    "print(len(cap_trn[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preperation checkpoint:\n",
    "\n",
    "If everything works well until here, one may use these cells to save the dataset and export them layer for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickle import dump\n",
    "# If everything works properly, this is a checkpoint \n",
    "inception_export = root_dir + '\\\\exports'\n",
    "\n",
    "if not os.path.exists(inception_export):\n",
    "    os.mkdir(inception_export)\n",
    "\n",
    "os.chdir(inception_export)\n",
    "dump(enc_trn, open('enc_trn.pkl', 'wb'))\n",
    "dump(enc_val, open('enc_val.pkl', 'wb'))\n",
    "dump(enc_tst, open('enc_tst.pkl', 'wb'))\n",
    "dump(cap_trn, open('cap_trn.pkl', 'wb'))\n",
    "dump(cap_val, open('cap_val.pkl', 'wb'))\n",
    "dump(cap_tst, open('cap_tst.pkl', 'wb'))\n",
    "os.chdir(root_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load data again:\n",
    "root_dir = os.getcwd()\n",
    "inception_export = root_dir + '\\\\exports\\\\'\n",
    "enc_trn = unpickle_data(inception_export, 'enc_trn.pkl')\n",
    "enc_val = unpickle_data(inception_export, 'enc_val.pkl')\n",
    "enc_tst = unpickle_data(inception_export, 'enc_tst.pkl')\n",
    "cap_trn = unpickle_data(inception_export, 'cap_trn.pkl')\n",
    "cap_val = unpickle_data(inception_export, 'cap_val.pkl')\n",
    "cap_tst = unpickle_data(inception_export, 'cap_tst.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enc_trn 285041\n",
      "cap_trn 285041\n",
      "(285041, 2048)\n",
      "(285041, 17)\n"
     ]
    }
   ],
   "source": [
    "# Check sizes on datasets - proofread material:\n",
    "#Tests\n",
    "print('enc_trn', len(enc_trn))\n",
    "print('cap_trn', len(cap_trn))\n",
    "\n",
    "enc_trn = np.array(enc_trn)\n",
    "cap_trn = np.array(cap_trn)\n",
    "\n",
    "print(enc_trn.shape)\n",
    "print(cap_trn.shape)\n",
    "\n",
    "enc_val = np.array(enc_val)\n",
    "cap_val = np.array(cap_val)\n",
    "\n",
    "enc_tst = np.array(enc_tst)\n",
    "cap_tst = np.array(cap_tst)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0498227  0.10677398 0.05705195 ... 0.75969905 0.36796963 0.        ]\n",
      "[  1  16  19   8   4  61 125 107  72  18  15   3   2   0   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "print(enc_trn[0])\n",
    "print(cap_trn[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Form output labels:\n",
    "def generate_labels(caps_set):\n",
    "    cap_lbl = []\n",
    "    for i in range(0, len(caps_set)):\n",
    "        temp = caps_set[i][1:]\n",
    "        temp = np.append(temp, 0)\n",
    "        cap_lbl.append(temp)\n",
    "    cap_lbl = np.array(cap_lbl)\n",
    "    return cap_lbl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(285041, 17)\n",
      "(285041, 17)\n",
      "[  1  16  19   8   4  61 125 107  72  18  15   3   2   0   0   0   0]\n",
      "[ 16  19   8   4  61 125 107  72  18  15   3   2   0   0   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "# Form label by shifting 1 unit. -testing:\n",
    "cap_trn_lbl = generate_labels(cap_trn)\n",
    "cap_val_lbl = generate_labels(cap_val)\n",
    "cap_tst_lbl = generate_labels(cap_tst)\n",
    "\n",
    "print(cap_trn.shape)\n",
    "print(cap_trn_lbl.shape)\n",
    "\n",
    "\n",
    "print(cap_trn[0])\n",
    "print(cap_trn_lbl[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO AFTER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create sequences of images, input sequences and output words for an image\n",
    "def create_sequences(tokenizer, max_length, descriptions, photos, vocab_size):\n",
    "\tX1, X2, y = list(), list(), list()\n",
    "\t# walk through each image identifier\n",
    "\tfor key, desc_list in descriptions.items():\n",
    "\t\t# walk through each description for the image\n",
    "\t\tfor desc in desc_list:\n",
    "\t\t\t# encode the sequence\n",
    "\t\t\tseq = tokenizer.texts_to_sequences([desc])[0]\n",
    "\t\t\t# split one sequence into multiple X,y pairs\n",
    "\t\t\tfor i in range(1, len(seq)):\n",
    "\t\t\t\t# split into input and output pair\n",
    "\t\t\t\tin_seq, out_seq = seq[:i], seq[i]\n",
    "\t\t\t\t# pad input sequence\n",
    "\t\t\t\tin_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
    "\t\t\t\t# encode output sequence\n",
    "\t\t\t\tout_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
    "\t\t\t\t# store\n",
    "\t\t\t\tX1.append(photos[key][0])\n",
    "\t\t\t\tX2.append(in_seq)\n",
    "\t\t\t\ty.append(out_seq)\n",
    "\treturn array(X1), array(X2), array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN Model Development:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_batch(data_enc, data_cap, batch_start_index, batch_size):\n",
    "    enc_batch = data_enc[batch_start_index:batch_start_index+batch_size]\n",
    "    \n",
    "    cap_batch = []\n",
    "    for i in range(batch_start_index,batch_start_index+batch_size):\n",
    "        cap_batch.append(data_cap[i])\n",
    "    return enc_batch, cap_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n",
      "512\n",
      "17\n",
      "[0.0498227  0.10677398 0.05705195 ... 0.75969905 0.36796963 0.        ]\n",
      "[  1   4  12   8   4   3  10 254   3  93   4 185   2   0   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "enc_batch, cap_batch = construct_batch(enc_trn, cap_trn, 0, 512)\n",
    "\n",
    "print(len(enc_batch))\n",
    "print(len(cap_batch))\n",
    "print(len(cap_batch[0]))\n",
    "\n",
    "print(enc_batch[1])\n",
    "print(cap_batch[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Dense, Dropout, LSTM, Embedding, GRU, add\n",
    "from tensorflow.compat.v1.keras.layers import CuDNNLSTM\n",
    "\n",
    "def lstm_block(enc_size, vocab_size, max_length, unit_size, drop_rate): # max_length: max length of captions\n",
    "    # feature extractor model - Already calculated reduce size.\n",
    "    tin = Input(shape=(enc_size,))\n",
    "    f1 = Dropout(drop_rate)(tin)\n",
    "    f2 = Dense(unit_size, activation='relu')(f1)\n",
    "    \n",
    "    # sequence model - this part is for the captions:\n",
    "    cin = Input(shape=(max_length,))\n",
    "    s1 = Embedding(vocab_size, unit_size, mask_zero=True)(cin)\n",
    "    s2 = Dropout(drop_rate)(s1)\n",
    "    s3 = LSTM(unit_size,return_sequences=True)(s2)\n",
    "    s4 = LSTM(unit_size,return_sequences=True)(s3)\n",
    "    s5 = LSTM(unit_size)(s4)\n",
    "    # decoder model\n",
    "    decoder1 = add([f2, s5])\n",
    "    decoder2 = Dense(unit_size, activation='relu')(decoder1)\n",
    "    outputs = Dense(max_length, activation='softmax')(decoder2)\n",
    "    \n",
    "    # tie it together [image, seq] [word]\n",
    "    model = tf.keras.Model(inputs=[tin,cin], outputs=outputs)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='Adam',metrics=[tf.keras.metrics.Accuracy()])\n",
    "    \n",
    "    # summarize model\n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 17)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 17, 256)      257024      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 17, 256)      0           embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            [(None, 2048)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     (None, 17, 256)      525312      dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 2048)         0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 17, 256)      525312      lstm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 256)          524544      dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   (None, 256)          525312      lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 256)          0           dense[0][0]                      \n",
      "                                                                 lstm_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          65792       add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 17)           4369        dense_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 2,427,665\n",
      "Trainable params: 2,427,665\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()  # For easy reset of notebook state.\n",
    "model = lstm_block(2048, 1004, 17, 256, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enc_trn          (285041, 2048)\n",
      "cap_trn          (285041, 17)\n",
      "cap_trn_lbl      (285041, 17)\n",
      "enc_val          (35684, 2048)\n",
      "cap_val          (35684, 17)\n",
      "cap_val_lbl      (35684, 17)\n"
     ]
    }
   ],
   "source": [
    "print('enc_trn         ', enc_trn.shape)\n",
    "print('cap_trn         ', cap_trn.shape)\n",
    "print('cap_trn_lbl     ', cap_trn_lbl.shape)\n",
    "print('enc_val         ', enc_val.shape)\n",
    "print('cap_val         ', cap_val.shape)\n",
    "print('cap_val_lbl     ', cap_val_lbl.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 30000 samples, validate on 35684 samples\n",
      "Epoch 1/20\n",
      "30000/30000 [==============================] - 43s 1ms/sample - loss: 392070163.9491 - accuracy: 0.3349 - val_loss: 1330977477.8038 - val_accuracy: 0.3435\n",
      "Epoch 2/20\n",
      "30000/30000 [==============================] - 29s 979us/sample - loss: 4601788538.0608 - accuracy: 0.3424 - val_loss: 9125773522.8607 - val_accuracy: 0.3435\n",
      "Epoch 3/20\n",
      "30000/30000 [==============================] - 22s 718us/sample - loss: 16463653892.3691 - accuracy: 0.3424\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-8c8491460623>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0menc_trn\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m30000\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcap_trn\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m30000\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcap_trn_lbl\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m30000\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0menc_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcap_val\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcap_val_lbl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\ayhok\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    727\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 728\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    729\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    730\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32mc:\\users\\ayhok\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[0;32m    368\u001b[0m                       \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTEST\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    369\u001b[0m                       \u001b[0mtraining_context\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0meval_context\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 370\u001b[1;33m                       total_epochs=1)\n\u001b[0m\u001b[0;32m    371\u001b[0m                   cbks.make_logs(model, epoch_logs, eval_result, ModeKeys.TEST,\n\u001b[0;32m    372\u001b[0m                                  prefix='val_')\n",
      "\u001b[1;32mc:\\users\\ayhok\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[1;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[0;32m    121\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[0;32m    122\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 123\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    124\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m         \u001b[1;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ayhok\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[1;34m(input_fn)\u001b[0m\n\u001b[0;32m     84\u001b[0m     \u001b[1;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[1;32m---> 86\u001b[1;33m                               distributed_function(input_fn))\n\u001b[0m\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ayhok\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 457\u001b[1;33m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    458\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_counter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcalled_without_tracing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ayhok\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    492\u001b[0m       \u001b[1;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    493\u001b[0m       \u001b[1;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 494\u001b[1;33m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    495\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    496\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[1;32mc:\\users\\ayhok\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1821\u001b[0m     \u001b[1;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1822\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1823\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1824\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1825\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ayhok\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1139\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[0;32m   1140\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[1;32m-> 1141\u001b[1;33m         self.captured_inputs)\n\u001b[0m\u001b[0;32m   1142\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1143\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ayhok\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1222\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1223\u001b[0m       flat_outputs = forward_function.call(\n\u001b[1;32m-> 1224\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[0;32m   1225\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1226\u001b[0m       \u001b[0mgradient_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_delayed_rewrite_functions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ayhok\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    509\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    510\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 511\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    512\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    513\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32mc:\\users\\ayhok\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[0;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                                                num_outputs)\n\u001b[0m\u001b[0;32m     62\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit([enc_trn[0:30000][:], cap_trn[0:30000][:]], cap_trn_lbl[0:30000][:], epochs=20, verbose=1, validation_data=([enc_val, cap_val], cap_val_lbl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
